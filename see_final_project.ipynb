{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WRRqRVD0xOpCuCfad419Ro56-78YX0XW",
      "authorship_tag": "ABX9TyNi9vrfadqKQ3ebr2e7ruOV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seerosem7/text_analysis_final_project/blob/main/see_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training NER to Process Alt-Right Data**\n",
        "\n",
        "\n",
        "****\n",
        "\n"
      ],
      "metadata": {
        "id": "MehKBp75pMfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "iKPjuJ-Sprzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Original Project\n",
        "  Project Veritas (PV) is a self-described citizen journalist organization founded by James O’Keefe that produces controversial, undercover content with an alt-right orientation. Since 2016, numerous scholars, particularly disinformation researchers, have used data from social media to map out alternative and right-wing media networks, including Starbird (2017) and Lewis (2018). Although PV and O’Keefe are situated within what Lewis (2018) calls the “alternative influence network,” it is unclear what the exact structure of their digital network is. Because PV is orientated towards informal “citizen” journalism, it lacks the concrete organizational structure of mainstream journalistic institutions. Rather than being anomalous, the \"slippery\" structure of PV is indicative of broader challenges in defining what exactly the \"alt-right\" is and who can be considered part of it (Crosset 2018). Boundaries between groups are often in flux, their ideologies mix and mingle, and their network structures and participants may overlap (Lewis 2018, Marwick and Lewis 2017).\n",
        "\n",
        "\n",
        "The fluid nature of alt-right digital communities poses challenges for qualitative digital researchers. In particular, digital ethnographers may struggle to define their \"field sites” in the same way that their physical counterparts can. Over the past year, I encountered this challenge when I was conducting a digital ethnography of Project Veritas, James O'Keefe, and their followers. This research project initially aimed to situate PV and O’Keefe within the alt-right ecosystem, and in doing so, map the “field site\" of my ethnography. In doing so, I intended to not only better define my own field site, but suggest a methodology that other digital ethnographers and qualitative researchers could build upon.\n"
      ],
      "metadata": {
        "id": "aB8RJbb9qpIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges and Project 2.0\n",
        "\n",
        "\n",
        "\n",
        "To map the network around O'Keefe and PV, I planned to scrape articles from the websites of Project Veritas and two other alt-right publishers - Breitbart and The Daily Caller. Then, I planned to run Named Entity Recognition (NER) using the Python library spaCy to build a database of individuals and organizations referenced in relation to PV and O'Keefe. Finally, I intended to develop a social network based on \"entity document co-occurrence,\" following Brandon Rose's model.\n",
        "\n",
        "However, I discovered that the spaCy library struggles to extract entities from data taken from alt-right publishers. The library spaCy is trained on large, annotated datasets taken from “news, broadcast, talk shows, weblogs, usenet newsgroups, and conversational telephone speech.\" It operates by making predictions about how entities appear based on what it has learned about how language functions from its training data. (Walsh, Intro to Cultural Analytics) This makes it a powerful tool for processing large amounts of text by recognizing and classifying well-known, meaningful entities, such as common names and brands.\n",
        "\n",
        "But because Spacy makes predictions based on its training data, it is less accurate at processing domain-specific textual data. (Kiselov 2022) As such, programmers working with domain-specific data may manually annotate a set of textual data and use this to train a custom language model, resulting in more accurate predictions. Alt-right content often includes its own domain-specific linguistic style and terminology. Content published on Breitbart, Project Veritas, and The Daily Caller reads much differently from content produced by professional journalists. Organizations and individuals are often referred to in shorthand, slang, or abbreviations. The way entities appear in the text is also inconsistent, with the same entities written differently throughout the article.\n",
        "\n",
        "Although I still intend to develop a social network of my fieldsite, doing so with the default spaCy model would result in a partial dataset. I decided to use this term paper to create a custom model of spaCy that I can later use to extract entities from my dataset and build a social network\n",
        "\n"
      ],
      "metadata": {
        "id": "cHZamPF_eTHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Significance\n",
        "***\n",
        "\n",
        "This study is significant for its subject matter as well as its methodology. Regarding its subject matter, O’Keefe and PV are influential actors in the alt-right ecosystem. O'Keefe's group has an outsize influence despite their small size. Their video-first content often “goes viral,” influencing the agenda of mainstream news sites and even impacting policymaking, as in the case of the ACORN controversy in 2009 (Dreier and Martin 2010). Despite the influence of O’Keefe, no scholars, to my knowledge, have mapped out this particular network of self-described journalists or identified their exact place in the broader alt-right news ecosystem.\n",
        "\n",
        "\n",
        "Methodologically, this study provides a potential toolkit for digital ethnographers and qualitative online researchers to map their networked field sites. In particular, it provides an example **of** how Named Entity Recognition models can be trained on domain-specific data. This is methodologically useful for researchers of internet subcultures and alternative or radical political movements, showing how language models can be trained to better recognize subgroup-specific references, individuals, organizations, and abbreviations.\n"
      ],
      "metadata": {
        "id": "t4s_muuuh9II"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research Questions\n",
        "***\n",
        "\n",
        "1. What individuals and organizations are mentioned frequently in articles produced by or about PV and O’Keefe?\n",
        "2. How are Project Veritas and James O’Keefe situated within the broader structure of the alt-right news ecosystem?\n",
        "3. How will training spaCy on domain specific articles impact the output of NER?\n",
        "\n",
        "## Hypotheses\n",
        "***\n",
        "\n",
        "1. I expect to find that the individuals and actors most frequently mentioned in conversation with PV and O’Keefe will fall into three categories:\n",
        "\n",
        " > 1A. Actors and organizations in the broader alt right news ecosystem\n",
        "\n",
        " > 1B. Liberal organizations and politicians who are the target of PV’s journalistic operations\n",
        "\n",
        " > 1C. Scattered mentions of individual “citizen journalists” and “whistleblowers” who produce PV’s content.\n",
        "\n",
        "2. I expect that my custom model of spaCy will be able to recognize a greater number of entities and organizations with a higher degree of accuracy than the default spaCy library.\n"
      ],
      "metadata": {
        "id": "X-426pVRs85n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods\n",
        "***"
      ],
      "metadata": {
        "id": "J8f8pqTE2RIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Gathering\n",
        "\n",
        "Data for this project consists of articles scraped from the websites of three alt-right publishers: Project Veritas, Breitbart, and the Gateway Pundit. Breitbart and the Gateway Pundit were selected because they are two prominent content producers in the alt-right ecosystem. They are also some of the few websites that allow scraping of articles and do not have paywalls. Other sites (Infowars, the Daily Caller, and the Epoch Times) were excluded because they were inaccessible.\n",
        "\n",
        "I included articles in my study that fell within the timeframe of January 1, 2018 through January 1, 2023. Because of its small size, PV produces less content than mainstream news sites. This five year time frame will allow me to capture a larger and more representative dataset.\n",
        "\n",
        "The methods of web scraping varied by site. Project Veritas does not have a traditional “archive” of articles on its website, but rather a collection of what it calls “landmark undercover investigations.” With the assistance of bardeen.ai I extracted and saved the links for each of these 43 articles.\n",
        "Both the Gateway Pundit and Breitbart have searchable and filterable archives. I pulled up all articles containing the tags \"James O'Keefe\" and \"Project Veritas\" and saved the links to each using bardeen.ai. I pulled 275 articles from Breitbart and 92 from the Daily Caller.\n",
        "\n",
        "I compiled these links into a separate CSV file for each publisher. I chose to save them in separate CSVs rather than a single CSV because each publisher's content contained unique patterns in how the scraped text was displayed, which became relevant in my data cleaning (next section). Next I used the requests library to extract the text from each link and save it as a new tab in the CSV. I then used Beautiful Soup to strip away the HTML format. My code was taken from what we learned during class lectures. I also consulted ChatGPT for some modifications when I was having difficulty saving each of my articles individually in the correct directory.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ev2RJoO2oIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#repeated each of the below steps for breitbart CSVs and daily caller CSVs"
      ],
      "metadata": {
        "id": "88SDHpQl-N8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c75e9e-c608-43c4-b5a2-0286b8d31dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Am5nxfNBFDyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veritas_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/final_project/veritas_urls.csv\", delimiter=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "E6POSE7F_dS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_article(url):\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    html_string = response.text\n",
        "    return html_string"
      ],
      "metadata": {
        "id": "nij2enA5_cj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veritas_df['text'] = veritas_df['url'].apply(scrape_article)"
      ],
      "metadata": {
        "id": "AtieeMlH_kNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/final_project/test\"\n",
        "os.makedirs(folder_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "5jtfnETKAKwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = 0\n",
        "for text in veritas_df['text']:\n",
        "    soup = BeautifulSoup(text)\n",
        "    article = soup.get_text()\n",
        "\n",
        "    id += 1\n",
        "    try:\n",
        "        with open(f\"folder_path{id}.txt\", \"w\") as file:\n",
        "            file.write(str(article))\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving article {id}: {e}\")"
      ],
      "metadata": {
        "id": "h577foMFAPwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veritas_df"
      ],
      "metadata": {
        "id": "Av08p-MKAt15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n",
        "These articles had a very messy format and took a long time to clean. I cleaned the data for each publisher (Veritas, Daily Caller, and Breitbart) separately because there were different patterns in the text of each publisher.\n",
        "\n",
        "I removed duplicate articles, removed stopwords, made all text lowercase, removed special characters, ran spellcheck, and finally lemmatized the data using parts of speech tags. I used the methods we had developed in class. I also consulted ChatGPT to help modify the scripts to iterate through each article in my folders. I used the Python os module as well as regular expressions to remove the irrelevant, non-article content at the head and foot of each article. The methods differed slightly for each publisher.\n"
      ],
      "metadata": {
        "id": "zceUu1V09npM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "bnTfhEk_DAEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove special characters - repeated separately for articles from each of the three publishers.\n",
        "#I asked ChatGPT to write a for loop that would run through each file in my folder and apply the remove special characters function\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/final_project/articles_all/veritas_articles'\n",
        "\n",
        "txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
        "\n",
        "# Define a function to remove special characters from a string\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "# Loop through each .txt file and remove special characters\n",
        "for txt_file in txt_files:\n",
        "    file_path = os.path.join(folder_path, txt_file)\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    cleaned_content = remove_special_characters(content)\n",
        "\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(cleaned_content)"
      ],
      "metadata": {
        "id": "Us9nwYv1CKm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Next I remove extra white space. As with the last step, I prompted ChatGPT to supply me with a code that would iterate through every file in my folder and remove white space.\n",
        "\n",
        "breitbart_folder_path = '/content/drive/MyDrive/Colab Notebooks/final_project/articles_all/veritas_articles'\n",
        "\n",
        "def remove_white_space(file_path):\n",
        "    # Read the contents of the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    content = re.sub(' +', ' ', content)\n",
        "\n",
        "    # Remove extra blank lines\n",
        "    content = re.sub('\\n\\s*\\n', '\\n\\n', content)\n",
        "\n",
        "    # Write the cleaned content back to the file\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n",
        "\n",
        "\n",
        "# Iterate through all .txt files in the folder\n",
        "for filename in os.listdir(breitbart_folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(breitbart_folder_path, filename)\n",
        "        remove_white_space(file_path)\n",
        "\n",
        "print(\"Text files cleaned successfully.\")"
      ],
      "metadata": {
        "id": "CuHHqmg3DSyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove capitalization\n",
        "\n",
        "for filename in os.listdir(daily_caller_folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(daily_caller_folder_path, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        modified_content = content.lower()\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(modified_content)"
      ],
      "metadata": {
        "id": "wM-ZE0wyGqc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I asked ChatGPT to create a spellcheck loop that will run through all of my files, spellcheck, and correct them.\n",
        "# I applied this to each publisher.\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def spellcheck_and_modify(file_path):\n",
        "    # Read the contents of the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "\n",
        "    words = content.split()\n",
        "    spell = SpellChecker()\n",
        "    misspelled = spell.unknown(words)\n",
        "    corrected_words = [spell.correction(word) if word in misspelled else word for word in words]\n",
        "    corrected_words = [word if word is not None else original_word for word, original_word in zip(corrected_words, words)]\n",
        "\n",
        "    content_corrected = ' '.join(corrected_words)\n",
        "\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(content_corrected)\n",
        "\n",
        "def process_files_in_folder(breitbart_folder_path):\n",
        "    # Iterate through all .txt files in the folder\n",
        "    for filename in os.listdir(breitbart_folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(breitbart_folder_path, filename)\n",
        "            spellcheck_and_modify(file_path)\n",
        "\n",
        "\n",
        "process_files_in_folder(breitbart_folder_path)\n"
      ],
      "metadata": {
        "id": "fze5uZo5GUpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove stopwords\n",
        "\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for filename in os.listdir(daily_caller_folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(daily_caller_folder_path, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        words = word_tokenize(content)\n",
        "\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        modified_content = ' '.join(filtered_words)\n",
        "\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(modified_content)"
      ],
      "metadata": {
        "id": "EOoypPc7G0UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove any duplicate files from each folder I used the following code (repeated for each of the three publishers) - used pandas and drop.duplicates\n",
        "veritas_folder_path = \"/content/drive/MyDrive/Colab Notebooks/final_project/articles_all/veritas_articles\"\n",
        "\n",
        "def remove_duplicates(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    df = pd.DataFrame({'lines': lines})\n",
        "\n",
        "    df_unique = df.drop_duplicates()\n",
        "\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(df_unique['lines'])\n",
        "\n",
        "def process_files_in_folder(folder_path):\n",
        "    for filename in os.listdir(veritas_folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(veritas_folder_path, filename)\n",
        "            remove_duplicates(file_path)\n",
        "\n",
        "process_files_in_folder(veritas_folder_path)"
      ],
      "metadata": {
        "id": "OvCHGhuKFl9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These articles contained a lot of irrelevant text at the top and bottom that was pulled when I made the requests\n",
        "# I manually identified patterns in the text data\n",
        "# An example of a pattern: Breitbart has non-article text that starts with \"COMMENTS/nPlease let us know if you are having issues with commenting\" and ends with \"Copyright 2023 Breitbart\"\n",
        "# I asked ChatGPT how I could go about removing this irrelevant content. It suggested the regular expressions library. It took a few iterations of prompts to get a script that worked.\n",
        "# I ran modified versions of this script based on patterns found in other areas of my data\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "breitbart_folder_path = '/content/drive/MyDrive/Colab Notebooks/final_project/articles_all/breitbart_articles'\n",
        "test_article_breitbart = '/content/drive/MyDrive/Colab Notebooks/final_project/articles_all/breitbart_articles/breitbart_articles_1.txt'\n",
        "\n",
        "start_lines_text = 'COMMENTS\\nPlease let us know if youre having issues with commenting\\n'\n",
        "end_line_text = 'Copyright 2023 Breitbart'\n",
        "\n",
        "def remove_end_content(test_article_breitbart, start_lines, end_line):\n",
        "    # Read the contents of the file\n",
        "    with open(test_article_breitbart, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Combine consecutive lines\n",
        "    concatenated_lines = ''.join(lines)\n",
        "\n",
        "    # Construct a regular expression to match the content between start and end lines\n",
        "    pattern = re.compile(re.escape(start_lines) + '(.*?)' + re.escape(end_line), re.DOTALL)\n",
        "\n",
        "    # Find the starting and ending indices of the content to remove\n",
        "    match = pattern.search(concatenated_lines)\n",
        "\n",
        "    if match:\n",
        "        start_index = match.start()\n",
        "        end_index = match.end()\n",
        "\n",
        "        # Remove the content between start and end lines\n",
        "        modified_content = concatenated_lines[:start_index] + concatenated_lines[end_index:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open(test_article_breitbart, 'w', encoding='utf-8') as file:\n",
        "            file.write(modified_content)\n",
        "\n",
        "def remove_end_content_folder(breitbart_folder_path, start_lines, end_line):\n",
        "    # Iterate through all .txt files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            remove_end_content(file_path, start_lines, end_line)\n",
        "\n",
        "\n",
        "remove_end_content_folder(folder_path, start_lines_text, end_line_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vblk46rxDvwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tag each word in each file with part of speech to prepare for lematizing\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def tag_pos_in_files(daily_caller_folder_path):\n",
        "    for filename in os.listdir(daily_caller_folder_path):  # Fix variable name here\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(daily_caller_folder_path, filename)\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            words = word_tokenize(content)\n",
        "            pos_tags = [get_wordnet_pos(word) for word in words]\n",
        "            word_pos_tuples = list(zip(words, pos_tags))\n",
        "\n",
        "            print(f\"File: {filename}\")\n",
        "            print(word_pos_tuples)\n",
        "            print(\"\\n\")\n",
        "\n",
        "tag_pos_in_files(daily_caller_folder_path)"
      ],
      "metadata": {
        "id": "rAJI9JmhHeA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize based on the POS tags\n",
        "\n",
        "def lemmatize_files(daily_caller_folder_path):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for filename in os.listdir(daily_caller_folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(daily_caller_folder_path, filename)\n",
        "\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            words = word_tokenize(content)\n",
        "\n",
        "            pos_tags = [get_wordnet_pos(word) for word in words]\n",
        "\n",
        "            lemmatized_words = [lemmatizer.lemmatize(word, pos=pos_tag) for word, pos_tag in zip(words, pos_tags)]\n",
        "\n",
        "            lemmatized_content = ' '.join(lemmatized_words)\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(lemmatized_content)\n",
        "\n",
        "lemmatize_files(daily_caller_folder_path)\n"
      ],
      "metadata": {
        "id": "lsJCHrTlH8GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running NER with spaCy's default large language model\n",
        "***\n",
        "\n",
        "\n",
        "\n",
        "When I was finished with data cleaning, I tried running spaCy on a sample of my dataset. I instantly encountered the problem that led me to redefine my project. Even though I was running spaCy's largest language model, many of the entities in my data were not being picked up. As can be seen below, spaCy failed to detect numerous entities, primarily \"organizations.\" This appears to be because PV, Breitbart, and the Daily Caller often shorten the names of organizations in atypical and inconsistent ways.\n",
        "\n",
        "Below is the set of organizations, institutions, and government agencies that spaCy should have detected in my \"test\" article and applied the \"ORG\" label to. However, these entities were not detected by spaCy.\n",
        "\n",
        "1.   DARPA [Defense Advanced Research Projects Agency]\n",
        "2. Project Veritas\n",
        "3. Department Defense [US Department of Defense]\n",
        "4. NAIAD [National Institute of Allergy and Infectious Diseases]\n",
        "5. Inspector general [Office of the Inspector General]\n",
        "6. NIH [National Institutes of Health]\n",
        "7. DOD [Department of Defense]\n",
        "8. USDR [US Digital Response]\n",
        "\n",
        "It is also apparent that the \"sensitivity\" of spaCy to changes in how individual names are listed is not high enough to detect the inconsistent ways that individuals are referred to in my dataset. For example, \"Dr. Fauci\" is correctly labeled, but \"Fauci\" by itself is not detected.\n",
        "\n",
        "These numerous oversights indicate that even the large language model of spaCy cannot accurately predict how named entities appear in my domain specific texts. Running this NER on my entire dataset would thus result in an incomplete and inaccurate representation of the named entities. This would make my map of the social network incomplete."
      ],
      "metadata": {
        "id": "Js3_N3NwnhCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Custom Named Entity Recognition Model\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "iWasNgVC1_G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process\n",
        "***\n",
        "\n",
        "Rather than using a default model, I decided to pivot my project and train a custom NER model. Training a model with my own datasets allows spaCy to better predict how entities will appear in the specific domain, and will ideally allow spaCy to detect named entities written in the inconsistent, shorthand terminology of alt-right articles.\n",
        "\n",
        "I started out by basing my method on Nisanth N's article, \"Training Custom NER\" (2020) in Towards Data Science. This is a bare-bones outline of the essential steps needed to train a model. To understand the process in greater depth, I consulted Deepak John Reji's YouTube and github tutorials. For further specificity, I asked ChatGPT to break down each step of code, troubleshoot errors, and/or modify Nisanth and Reji's lines of code when I encountered challenges. I built my model on top of spaCy's preloaded large language model.\n",
        "\n",
        "The most time consuming aspect of this process resulted from mistakenly running incompatible versions of Python libraries. The tutorials I consulted were based on an older version of spaCy that required training data to be in tuple format. However, the newer version, 3.7.2, required \"example\" format. I also ran into issues with the version of spaCy's large language model that I had loaded. I used ChatGPT to very slowly rectify these version issues. While my code ultimately worked, there are likely redundant and/or inefficient sections of it due to many rounds of troubleshooting back and forth with ChatGPT.\n",
        "\n",
        "\n",
        "## Data Annotation\n",
        "***\n",
        "\n",
        "NER models are trained on datasets that are annotated by hand to indicate where specific entities appear in a body of text. The first step to training a model is thus to annotate domain specific data so that it can be uploaded for training. Annotating entails identifying an entity, applying a label (ie: person) to that entity, and indicating numerically where the entity appears in the text.\n",
        "\n",
        "I used an open-source annotator, https://tecoholic.github.io/ner-annotator/, to annotate my data in tuple format. I started by annotating one test article (previously pictured) in my datatest with \"PERSON\" and \"ORG tags. Once this test article was annotated, I saved it in JSON format and followed Deepak John Reji's YouTube tutorial to ensure it was in the correct tuple format."
      ],
      "metadata": {
        "id": "r7C259cuzfPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following scripts are based on Deepak John Reji's YouTube tutorial about annotating NER training data.\n",
        "# I asked ChatGPT to explain Reji's code to me so I would have a better understanding of what it was doing.\n",
        "# When I ran into errors in my code, I asked ChatGPT to identify the errors and modify the lines of code\n",
        "\n",
        "! pip install spacy==3.7.2\n",
        "import spacy\n",
        "from __future__ import unicode_literals, print_function\n",
        "!pip install plac\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "\n",
        "!pip install -U spacy\n",
        "!pip install spacy-lookups-data\n",
        "!python -m spacy download en\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg"
      ],
      "metadata": {
        "id": "oBq2yn342GQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import annotated test training article in JSON format\n",
        "\n",
        "json_path = '/content/drive/MyDrive/Colab Notebooks/final_project/train_data/annotations_org_1.json'\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "WrhduMlW2ZSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loop makes sure that the JSON data is in the correct tuples format to be readable by the model\n",
        "\n",
        "entity_name = \"ORG\"\n",
        "\n",
        "train_data = data['annotations']\n",
        "train_data = [tuple(i) for i in train_data]\n",
        "for i in train_data:\n",
        "    if i[1]['entities'] == []:\n",
        "        i[1]['entities'] = (0, 0, entity_name)\n",
        "    else:\n",
        "        i[1]['entities'][0] = tuple(i[1]['entities'][0])\n",
        "\n",
        "for i in train_data:\n",
        "    if i[1]['entities'] == []:\n",
        "        i[1]['entities'] = (0, 0, entity_name)\n",
        "    else:\n",
        "        i[1]['entities'][0] = tuple(i[1]['entities'][0])"
      ],
      "metadata": {
        "id": "EiMobbI63ZPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "jB3Evw8lBVRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transitioning to training the model, building on the large language model\n",
        "\n",
        "model = en_core_web_lg\n",
        "output_dir=Path(\"/content/drive/MyDrive/Colab Notebooks/final_project/conNER\")\n",
        "n_iter=100"
      ],
      "metadata": {
        "id": "eVcO8YKg3ybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"en_core_web_lg\"\n",
        "\n",
        "if model_name is not None:\n",
        "    nlp = spacy.load(model_name)\n",
        "    print(\"Loaded model '%s'\" % model_name)\n",
        "else:\n",
        "    nlp = spacy.blank('en')\n",
        "    print(\"Created blank 'en' model\")\n",
        "\n",
        "# Set up the pipeline\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner, last=True)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgvhUXKx4CrZ",
        "outputId": "eb07cdf5-7f5e-4f42-dbbb-6743d4db28fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model 'en_core_web_lg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =  ('/content/drive/MyDrive/Colab Notebooks/final_project/conNER')\n",
        "n_iter=100\n",
        "custom_model_name = \"conNER\"\n",
        "\n",
        "\n",
        "if output_dir is not None:\n",
        "    output_dir = Path(output_dir)\n",
        "    if not output_dir.exists():\n",
        "        output_dir.mkdir()\n",
        "    custom_model_path = output_dir / custom_model_name\n",
        "    nlp.to_disk(custom_model_path)"
      ],
      "metadata": {
        "id": "ZCLBh22J4S3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I initially ran this loop in tuple format, but the newer version of spaCy requires \"example\" format\n",
        "# ChatGPT provided a modified loop that processes my tuples into examples that can be read by the model\n",
        "\n",
        "from pathlib import Path\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Assuming train_data is a list of tuples (text, annotations)\n",
        "examples = []\n",
        "\n",
        "for text, annotations in train_data:\n",
        "    example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "    examples.append(example)\n",
        "\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    optimizer = nlp.begin_training()\n",
        "    for itn in range(n_iter):\n",
        "        random.shuffle(examples)\n",
        "        losses = {}\n",
        "        for example in tqdm(examples):\n",
        "            nlp.update(\n",
        "                [example],  # Pass a list containing the Example object\n",
        "                drop=0.5,\n",
        "                losses=losses\n",
        "            )\n",
        "        print(losses)\n",
        "\n",
        "\n",
        "# Save the model after the entire training loop\n",
        "output_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/final_project/conNER\")\n",
        "nlp.to_disk(output_dir / \"conNER_updated\")"
      ],
      "metadata": {
        "id": "jVHyg1_vVK53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Model\n",
        "***\n",
        "After this first round of training, I annotated the entities on one more article and ran it through my model. With two sets of training data processed, conNER (or Conservative Named Entity Recognition) was ready to be tested. In the following code, I ran conNER on the same test article that I used as an example of spaCy's lacking prediction ability earlier. I wanted a direct comparison of how the model's predictions have changed as a result of the training."
      ],
      "metadata": {
        "id": "NlkWv4ViArFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_lg\n",
        "import pandas as pd\n",
        "pd.options.display.max_rows = 600\n",
        "pd.options.display.max_colwidth = 400\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import pprint\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "_b5by4eJCfsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/final_project/conNER\""
      ],
      "metadata": {
        "id": "iLdjJS6iChM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(model_path)"
      ],
      "metadata": {
        "id": "ZioLARbdCvwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"/content/drive/MyDrive/Colab Notebooks/final_project/articles_all/article_1.txt\"\n",
        "text = open(filepath, encoding='utf-8').read()\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "yefv1WUACyi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, jupyter=True, style=\"ent\")"
      ],
      "metadata": {
        "id": "eHy2UILvC4U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Output\n",
        "***\n",
        "Despite only being trained on two article's worth of domain-specific data, conNER was noticeably more accurate about detecting entities than the default spaCy model. I ran the custom model, conNER, on the same test article as I ran the spaCy default model on. It correctly tagged all of the organizations that were not identified by the default model. These include:\n",
        "\n",
        "1. DARPA\n",
        "2. Project Veritas\n",
        "3. Department Defense\n",
        "4. Inspector General\n",
        "5. NAIAD\n",
        "6. NIH\n",
        "7. USDR\n",
        "8. DOD\n",
        "\n",
        "However, conNER incorrectly identified \"fauci testimony\" as an organization, rather than labeling \"fauci\" as an individual. It also labeled \"Washington DC\" as an organization. This indicates that, while the custom model may be better at correctly identifying organizations, it is prone to false positives that identify individuals as organizations. Given that I have only trained it on one \"PERSON\" annotated article and one \"ORG\" annotated article, these are encouraging results. Further training would likely increase the model's ability to predict named entities and make its identification more accurate."
      ],
      "metadata": {
        "id": "4AgA3dokC_oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions, Limitations, and Future Directions\n",
        "***\n",
        "\n",
        "Although my custom language model is only in its preliminary stages, the early results are promising. The model is able to more accurately predict when organizations appear in the text in abbreviated or atypical ways - such as \"DOD\" for Department of Defense or \"Inspector General\" for \"Office of the Inspector General.\" This shows increased fluency in the terminology of alt-right news sites, which often shirk the formal linguistic styles and terminology of professional news organizations.\n",
        "\n",
        "\n",
        "However, conNer appears to be over-inclusive in what it identifies as an \"organization,\" such as labeling \"Fauci testimony\" as an organization. This tendency to over-identify named entities as \"organizations\" might be rectified with further training. So, while HQ2 appears correct, the model is not complete enough to run my initially proposed analysis.\n",
        "\n",
        "\n",
        "As described in my introduction, I started this project with research questions about the nature of the network around Project Veritas and James O'Keefe. My original goal was to map the social network of this self-described citizen journalism organization. However, issues with the spaCy NER model's ability to accurately predict entity occurrence in my domain datasets complicated my objectives and methods.\n",
        "\n",
        "\n",
        "At this point, my custom model is limited. I have not had the time to adequately train conNER on enough data where I feel comfortable applying it to my entire corpus of data. I intend to train conNer on a larger number of  datasets annotated with \"PERSONS\" and \"ORGS\" to increase its predictive accuracy. Once I am satisfied with the model's predictions, I intend to run my initially proposed analysis, encompassing research questions RQ1 and RQ2 and hypotheses H1, H1A, H1B, and H1C.\n",
        "\n",
        "\n",
        "Because I am so new to coding, I built my custom NER model with the assistance of a variety of tutorials and ChatGPT suggestions. I encountered many errors during the programming process and primarily used ChatGPT to assist me in fixing them. Although ChatGPT was a very useful tool,  I think it is likely that relying on its suggestions may have made my code clunkier and more inconsistent than it could have been if I had the skill set to program it from scratch. Thus, the major limitation of my study  is that I am not fully confident in the quality of my code. To confidently assess and deploy my model, I need to improve both my conceptual knowledge of coding and my technical skills.\n",
        "\n",
        "\n",
        "Despite these limitations, my study provides some methodological guidance for digital ethnographers and qualitative researchers seeking to map the networked field sites they are studying. Natural language processing is a powerful tool. However, because it operates by prediction based on given datasets, it is less adept at parsing the terminology of specific internet or political subcultures. Training custom models provides a way for researchers of digital communities and subcultural, fringe, or extremist groups to harness more accurate language models.\n",
        "\n",
        "\n",
        "Moving forward in my own ethnographic work, I look forward to integrating natural language processing as a way to map the digital field sites I am studying.Improving my custom language model will allow me to map the social network of my ethnographic fieldsite.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6veIhdUwKcva"
      }
    }
  ]
}